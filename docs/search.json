[
  {
    "objectID": "gen_art/coastlines/index.html",
    "href": "gen_art/coastlines/index.html",
    "title": "Coastlines",
    "section": "",
    "text": "I created this system for the Genuary 2024 prompt “In the style of Vera Molnár” - it is inspired by her work La Ciotat. Since La Ciotat is also a French coastal town, as homage to Molnár’s work, I’ve named my pieces after some of the coastal locations that I’ve visited in the UK. The Lindisfarne pieces are the most similar to La Ciotat. The name of the system as a whole - Coastlines - also references the thin, vertical lines that are the building blocks of each piece as well as the frequent naturalistic imagery (e.g., hills or beaches).\nLicense: CC BY-NC-ND 4.0\nSet 1: Lindisfarne\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSet 2: Holyrood Park\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSet 3: Pentland Hills\n\n\n\n\n\n\n\n\n\n\n\n\nSet 4: Whitley Bay\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSet 5: Tynemouth\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSet 6: Llanddwyn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSet 7: North Shields"
  },
  {
    "objectID": "gen_art/apertures/index.html",
    "href": "gen_art/apertures/index.html",
    "title": "Apertures",
    "section": "",
    "text": "This system is based on the concepts from session 2 of the Art from Code generative art tutorial.\nLicense: CC BY-SA 4.0\nSet 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSet 2\n\n\n\n\n\n\n\n\n\n\n\n\nSet 3"
  },
  {
    "objectID": "gen_art/drifts/index.html",
    "href": "gen_art/drifts/index.html",
    "title": "Drifts",
    "section": "",
    "text": "This system explores the concepts from session 2 of the Art from Code generative art tutorial. It uses pattern and noise generators in the ambient package. My implementation combines Worley noise and waves, with settings that control the frequencies, noise seeds, where waves begin, and how much the waves contribute to the final pattern.\nLicense: CC BY-SA 4.0"
  },
  {
    "objectID": "data_science/seizure-states/index.html#overview",
    "href": "data_science/seizure-states/index.html#overview",
    "title": "Epilepsy research: Modulations in seizure states",
    "section": "Overview",
    "text": "Overview\nIn another post, I described my work characterising seizures as pathways and investigating how these pathways change over time. One disadvantage of this approach is that it’s tricky to quantify what parts of pathways change from seizure to seizure. For example, we might want to ask whether seizures that occur at a certain time of day all share a certain feature. I therefore developed a complementary approach: describing seizures as state progressions, where each state in a seizure captures a particular pattern of brain activity. I could then quantify which states appeared in each seizure (state occurrence) and how long each state lasted (state duration).\n\n\n\n\n\n\n\nSchroeder et al. 2023, Brain Communications\n\n\nI applied this approach to a unique dataset of chronic (spanning multiple months) recordings of brain activity in ten people with epilepsy, allowing me to investigate how seizure states change over days, weeks, and months. For example, the occurrence of many states changed over the course of a subject’s recording:\n\n\n\n\n\n\n\nSchroeder et al. 2023, Brain Communications\n\n\nI was particularly interested in cyclical changes in seizure states, as seizure risk is known to vary over daily and multi-day cycles. I therefore extracted cycles in a biomarker of pathological brain activity and compared seizure states to these cycles. This analysis revealed that seizure states often vary cyclically.\n\n\n\n\n\n\n\nSchroeder et al. 2023, Brain Communications\n\n\nUnderstanding how seizures change over different timescales could lead to new treatments that adapt over time to control different types of seizures."
  },
  {
    "objectID": "data_science/seizure-states/index.html#my-contributions",
    "href": "data_science/seizure-states/index.html#my-contributions",
    "title": "Epilepsy research: Modulations in seizure states",
    "section": "My contributions",
    "text": "My contributions\nThis project was part of my PhD thesis; I was responsible for shaping the project’s direction, undertaking the analyses, and communicating the findings. The brain recording data was obtained and organised by collaborators from another research institution, who also provided feedback throughout the project."
  },
  {
    "objectID": "data_science/seizure-states/index.html#data-science-approaches",
    "href": "data_science/seizure-states/index.html#data-science-approaches",
    "title": "Epilepsy research: Modulations in seizure states",
    "section": "Data science approaches",
    "text": "Data science approaches\n\nSignal processing: I used signal processing techniques such as filtering to preprocess brain signals.\nNetwork analysis: I described seizure dynamics as the time-varying network interactions between pairs of brain regions.\nClustering and dimensionality reduction: I used non-negative matrix factorisation as a soft-clustering method to extract recurring seizure states from time-varying seizure network interactions.\nTime series decomposition: I extracted cycles in brain activity using empirical mode decomposition, which decomposes a time series into oscillations at different frequencies. Unlike many other frequency decomposition approaches, empirical mode decomposition does not require the extracted cycles to be regular; it can capture prominent cycles even if the cycle period varies (e.g., if the cycle peaks every five to seven days instead of exactly every six days).\nCircular and non-parametric statistics: To characterise cycles in seizure states, I used a range of statistics, including Wilcoxon rank sum tests, phase locking values, and circular-linear correlation."
  },
  {
    "objectID": "data_science/epilepsy-abnormalities/index.html",
    "href": "data_science/epilepsy-abnormalities/index.html",
    "title": "Epilepsy research: Do brain abnormalities change over time?",
    "section": "",
    "text": "The full details for this project are available in our open access paper, Temporal stability of intracranial electroencephalographic abnormality maps for localizing epileptogenic tissue. My Python code for generating the main figures is available on my GitHub."
  },
  {
    "objectID": "data_science/epilepsy-abnormalities/index.html#overview",
    "href": "data_science/epilepsy-abnormalities/index.html#overview",
    "title": "Epilepsy research: Do brain abnormalities change over time?",
    "section": "Overview",
    "text": "Overview\nWhen seizures are not controlled by medication, one of the few remaining treatment options is brain surgery to remove the part of the brain that causes seizures. However, identifying those regions can be difficult, and people with epilepsy often continue having seizures after surgery. The main focus of my postdoc research is developing computational measures that help pinpoint pathological brain regions so that they can successfully be removed.\nThis study investigated the stability of one of our measures of brain “abnormalities” that we use to identify pathological brain regions. Our measure is computed from intracranial EEG, which is recorded from electrodes that are temporarily implanted directly on or in the brain. These recordings are usually multiple days and capture a variety of brain activity, including periods of sleep, wake, and seizures. To use our abnormality measure clinically, we first need to know whether the measure is robust. How much is it influenced by the type of brain activity? How much data is needed to reliably compute our measure?\nTo determine the impact of removing brain abnormalities, we additionally computed a summary metric, DRS, that captures differences in abnormality levels between the resected (i.e., hypothesised to be pathological) and spared (thought to be healthy) brain regions. We expected DRS to differ depending on whether the subject was seizure free after surgery – indicating that the hypothesised pathological regions indeed caused seizures – or not seizure free.\nWe found that although there is some variability in abnormalities at the level of different brain regions, the relationship between resected and spared brain regions (as captured by the metric DRS) remained relatively consistent over time in each subject. Additionally, we could use DRS to distinguish subjects that were seizure free versus not seizure free after surgery.\nWe used a variety of plots to visualise the data at the level of individual subjects as well as the entire cohort - I’ve included a couple examples here. This figure shows that brain abnormalities and the summary DRS metric were similar in time periods close to seizures (“peri-ictal”) and far away from seizures (“interictal”). Subjects were divided by whether they had good (ILAE 1, blue) or poor (ILAE 2-5, red) treatment outcomes.\n\n\n\n\n\n\n\nWang et al. 2023, Brain Communications\n\n\nThe figure below summarises results across subjects. We show the distribution of our summary measure in each subject and then used a machine learning classification metric to evaluate whether our measure distinguished subjects by their surgical outcomes.\n\n\n\n\n\n\n\nWang et al. 2023, Brain Communications"
  },
  {
    "objectID": "data_science/epilepsy-abnormalities/index.html#my-contributions",
    "href": "data_science/epilepsy-abnormalities/index.html#my-contributions",
    "title": "Epilepsy research: Do brain abnormalities change over time?",
    "section": "My contributions",
    "text": "My contributions\nThis project was a collaborative effort between five members of the lab as well as our clinical collaborators. Throughout the project, we met regularly to discuss preliminary results and brainstorm next steps. I was heavily involved in the analysis and writing. I worked with two other lab members to process and analyse the brain recording data, then drafted the manuscript’s methods, results, and supplementary material. I also generated most of the figures and final documented code, with feedback from the group."
  },
  {
    "objectID": "data_science/epilepsy-abnormalities/index.html#data-science-approaches",
    "href": "data_science/epilepsy-abnormalities/index.html#data-science-approaches",
    "title": "Epilepsy research: Do brain abnormalities change over time?",
    "section": "Data science approaches",
    "text": "Data science approaches\n\nData wrangling: Our lab has collaboratively created a large database of iEEG data along with patient, recording, and brain region metadata. A subset of this data was used and further organised for this project.\nSignal processing and time series frequency decomposition: We analysed brain activity in the frequency domain by computing power spectral densities and band power.\nNormative mapping: To determine if brain regions were abnormal, we compared brain activity in each patient to a normative map of our measure. This map describes normal brain dynamics in each brain region and was created using non-pathological brain regions from over 200 subjects from a separate iEEG study. Importantly, the normative map accounts for the well-known spatial variability in brain activity, allowing us to compare brain regions with different expected profiles of activity.\nMachine learning classification metrics: We defined our summary measure, DRS, as the area under the curve (AUC) for distinguishing resected and spared brain regions using abnormalities. We also used AUC to quantify whether DRS distinguishes patients who had good versus poor treatment outcomes.\nNon-parametric statistics: We used Wilcoxon rank sum tests to test whether our metric DRS significantly different between patients with good and patients with poor treatment outcomes.\nDimensionality reduction and time series decomposition: As part of the exploratory phase of this project, we also used approaches such as non-negative matrix factorisation and empirical mode decomposition to investigate possible spatiotemporal patterns in patient abnormalities."
  },
  {
    "objectID": "data_science/seizure-pathways/index.html",
    "href": "data_science/seizure-pathways/index.html",
    "title": "Epilepsy research: Variability in seizure pathways",
    "section": "",
    "text": "This work is published in Seizure pathways change on circadian and slower timescales in individual patients with focal epilepsy and Multiple mechanisms shape the relationship between pathway and duration of focal seizures."
  },
  {
    "objectID": "data_science/seizure-pathways/index.html#overview",
    "href": "data_science/seizure-pathways/index.html#overview",
    "title": "Epilepsy research: Variability in seizure pathways",
    "section": "Overview",
    "text": "Overview\nAlthough it is well-established that brain dynamics vary over time, we are just beginning to understand how that variability impacts neurological disorders. In people with epilepsy, fluctuations in brain dynamics impact not only when seizures occur but also features such as symptoms and seizure spread. My PhD research focused on visualising and quantifying this variability, with ultimate goal of inspiring treatments that adapt to changing brain dynamics.\nQuantitatively comparing seizures is challenging because seizures are such complex events. They have both spatial and temporal features: they change the activity of multiple brain regions, and that activity evolves from seizure start to finish. Seizures therefore need to be described using multivariate time series that capture spatiotemporal changes in brain dynamics. I described these time series as pathways through the space of possible brain dynamics.\nI visualised seizure pathways in two dimensions using a dimensionality reduction technique that tries to maintain high dimensional distances in a lower dimensional space; it essentially squishes the data into two dimensions so that it’s easier to visually compare different observations. This representation isn’t perfect, but it’s a good way to start investigating the structure and relationships in complex data.\nHere’s an example of this approach using one subject’s seizures. Each point represents the brain’s activity during a short part of a seizure. Points that are closer together represent similar patterns of brain activity. By connecting points in the same seizure, we see how brain activity changes during the seizure; this line is the seizure’s pathway.\n\n\n\n\n\nHere are more seizures from the same subject - you can see the seizure variability from both the brain recordings (A) and seizure pathways (C). I also quantified the “distance” or dissimilarity between each pair of pathways (D) to have an objective comparison of seizure pathways for downstream analysis.\n\n\n\n\n\n\n\nSchroeder et al. 2020, PNAS\n\n\nOne of my first observations was that different types of seizure pathways do not occur randomly - instead, more similar seizures tended to occur close together in time. For example, in (A) below, the purple seizure pathways migrated through network space as time passed, with similar pathways (e.g., seizures 6-8) occurring back-to-back. The rest of the figure quantifies the relationship between seizure pathways and their “temporal distance”: the amount of time between each pair of seizures.\n\n\n\n\n\n\n\nSchroeder et al. 2020, PNAS\n\n\nImportantly, quantifying seizure variability paves the way for additional studies by providing an objective measure of seizure (dis)similarity. For example, in my next study I asked if seizures with similar pathways also last a similar amount of time. I found that these different features weren’t necessarily linked (as shown by the examples below), which suggests that seizure pathways and durations can be altered by different mechanisms.\n\n\n\n\n\n\n\nSchroeder et al. 2022, Brain Communications"
  },
  {
    "objectID": "data_science/seizure-pathways/index.html#my-contributions",
    "href": "data_science/seizure-pathways/index.html#my-contributions",
    "title": "Epilepsy research: Variability in seizure pathways",
    "section": "My contributions",
    "text": "My contributions\nMy PhD research involved independent projects under the guidance of my supervisor. I undertook the analyses, communicated the results in papers and presentations, and drove the research directions. Throughout the projects, I incorporated feedback from the lab and our clinical collaborators."
  },
  {
    "objectID": "data_science/seizure-pathways/index.html#data-science-approaches",
    "href": "data_science/seizure-pathways/index.html#data-science-approaches",
    "title": "Epilepsy research: Variability in seizure pathways",
    "section": "Data science approaches",
    "text": "Data science approaches\n\nData wrangling: For this project, I extracted and organised seizure brain recordings, seizure metadata, and subject metadata.\nSignal processing: I used signal processing techniques such as filtering to preprocess brain signals.\nNetwork analysis: I described seizure dynamics as the time-varying functional connectivity between pairs of brain regions. Functional connectivity is a type of network that describes the similarity of pairs of time series (e.g., brain activity). I used coherence to quantify the similarity of pairs of brain signals in the frequency domain. I described each seizure’s time-varying network evolution as a pathway through the high dimensional network space.\nDimensionality reduction: I used non-negative matrix factorisation to reduce the dimensionality of the seizure networks and multidimensional scaling to visualise changes in seizure dynamics in a two dimensional space.\nTime series analysis: Seizure pathways are multivariate time series. To compare these time series, I used dynamic time warping and distance metrics. Dynamic time warping stretches pairs of time series to align similar dynamics, making it a useful approach for comparing time series that evolve at different rates. For example, two seizures may spread in a similar pattern, but at different rates. Dynamic time warping allows us to recognise their similar pathway, even though they progress through the pathway at different speeds.\nNon-parametric statistics: I used permutation tests to test for associations between seizure pathways and other seizure features.\nSimulations and modelling: I compared the observed changes in seizure pathways to a simple model of how seizures could change over different timescales.\nClustering: I used clustering approaches such as hierarchical clustering and k-means clustering to cluster seizures based on their features, as well as cluster evaluation metrics to determine the optimal number of clusters."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Gabrielle M. Schroeder\n\n\nData Science - Data Visualisation - Software Engineering\n\n\n  \n    \n  \n  \n\nHi, I’m Gabrielle. I am a research software engineer and data visualisation enthusiast from Newcastle upon Tyne, UK.\nRead more about me or see examples of my data science, data visualisation, and generative art projects."
  },
  {
    "objectID": "software/rsecon24-requirements/index.html",
    "href": "software/rsecon24-requirements/index.html",
    "title": "RSECon24: Requirements capture",
    "section": "",
    "text": "As a research software engineer, one of my key responsibilities is determining a project’s requirements based on the needs and priorities of my collaborators. At the 2024 Research Software Engineering Conference (RSECon24), I had the opportunity to present this talk on requirements capture, using one of our team’s more complex projects as an example."
  },
  {
    "objectID": "software/rsecon24-requirements/index.html#slides",
    "href": "software/rsecon24-requirements/index.html#slides",
    "title": "RSECon24: Requirements capture",
    "section": "Slides",
    "text": "Slides\nDownload PDF file."
  },
  {
    "objectID": "software/rsecon24-requirements/index.html#abstract",
    "href": "software/rsecon24-requirements/index.html#abstract",
    "title": "RSECon24: Requirements capture",
    "section": "Abstract",
    "text": "Abstract\nSolving the right problems: Requirements capture for large-scale, evolving research software\nA key challenge in designing sustainable research software is evaluating and meeting ongoing user needs. OpenScan is a National Institute for Health and Care Research Innovation Observatory (NIHRIO) project that collects, standardises, analyses, and publishes international clinical trial and medical device data. Since OpenScan’s initial development, NIHRIO has faced challenges with maintaining, expanding, and fully utilising the project for their research, in part due to its complex technical stack and Amazon Web Services (AWS) infrastructure.\nNIHRIO enlisted the Newcastle University RSE Team to update and re-design components of OpenScan to address these issues. We began by evaluating and documenting the design, including how NIHRIO currently uses OpenScan. We then captured the long-term requirements of NIHRIO’s developers and researchers. This process highlighted that, in addition to NIHRIO’s stated requirements, we needed to address challenges with account management, documentation, and reproducible workflows.\nIn this talk, we discuss capturing OpenScan2’s requirements using interviews, user stories, and work packages. We highlight one of the workflow challenges that we uncovered: updating the web scrapers that collect OpenScan’s data. Finally, we show how we designed new solutions to address these barriers.\nOpenScan’s redesign demonstrates the importance of requirements capture when developing and (re-)designing research software. Ultimately, ensuring we solve the right problems will help make OpenScan a sustainable resource that enables innovative research on medical progress."
  },
  {
    "objectID": "blog/intro_pyart1.html",
    "href": "blog/intro_pyart1.html",
    "title": "Introduction to generative art in Python using Perlin noise",
    "section": "",
    "text": "I’ve struggled to find many introductory generative art tutorials using Python data science libraries (e.g., matplotlib), so I decided to write my own!\nGenerative art provides great opportunities for learning new mathematical and programming skills. Although I primarily use R for generative art, I mainly use Python at work, and I wanted a resource I could share with other Python practitioners. This tutorial is inspired by Danielle Navarro’s Art from Code tutorial and Yvan Scher’s archipelagos tutorial"
  },
  {
    "objectID": "blog/intro_pyart1.html#noise-in-generative-art",
    "href": "blog/intro_pyart1.html#noise-in-generative-art",
    "title": "Introduction to generative art in Python using Perlin noise",
    "section": "Noise in generative art",
    "text": "Noise in generative art\nGenerative art is art that is at least partly created by a non-human system, such as a computer program. The artist can build in certain rules that the system has to follow, but at least some decisions are made by the system itself.\nOne of the primary approaches in generative art is creating data using “noise” to produce shapes or patterns. There are many ways to generate noise with different characteristics, and the approach we’ll use here is Perlin noise. Here’s a two dimensional image made using Perlin noise:\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, this image has a lot of structure - nearby values tend to be similar, creating islands or blobs of higher and lower values.\nTo make the image above, I used the Python noise package. We’ll also need numpy for handling our arrays of data and the visualisation library matplotlib.\n\nimport noise\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nTo generate two dimensional Perlin noise like the image above, we can use the pnoise2 function in the noise package. pnoise2 takes two values (e.g., x and y coordinates) and returns Perlin noise generated from those values.\n\nx = 0.001\ny = 0.001\nperlin_value = noise.pnoise2(x, y)\nprint(perlin_value)\n\n0.0009999900357797742\n\n\nWe need to generative lots of values for different points, so lets make a function that creates an array filled with Perlin noise. We’ll use the indices of the array - i.e., the row and column numbers - as the inputs for pnoise2.\n\ndef gen_perlin_2d(n_x, n_y, scale=1, **kwargs):\n  \n  # Create an array for holding Perlin noise\n  # y coordinates correspond to the rows\n  perlin_data = np.zeros((n_y, n_x))\n  \n  # Largest dimension - use to scale values\n  max_n=max(n_y, n_x)\n\n  # Compute noise using indices of the array\n  for i in range(n_y): \n    for j in range(n_x):\n      perlin_data[i,j] = noise.pnoise2(\n        j/max_n * scale, # pass x value first (j)\n        i/max_n * scale,\n        **kwargs # Additional keyword arguments to pass to pnoise2\n      )\n      \n  return perlin_data\n\nYou probably noticed that I didn’t just pass the array’s indices, i and j, to pnoise2 - I’ve transformed them based on the size of the array and the scale argument. This approach will make it easier to use to control the features of our art - I’ll talk more about these data transformations below.\nFirst, though, we also need a way to visualise our art. We can use matplotlib’s imshow function to make a heatmap of the data:\n\ndef plot_perlin_art(perlin_data, show_plot=True):\n  \n  # Plot the data as a heatmap (each value = different colour)\n  fig, ax = plt.subplots(1, 1)\n  ax.imshow(perlin_data)\n  ax.axis(\"off\") # remove axes\n  ax.set_aspect(\"equal\") # equal aspect ratio\n  if show_plot:\n    plt.show()\n  \n  # Return figure and axes handles\n  return fig, ax\n\nNow we’re ready to make and plot some Perlin noise!\n\n# Define dimensions of the data\nn_x = 1000\nn_y = 1000\n\n# Generate data\nperlin_data = gen_perlin_2d(n_x, n_y)\n\n# Plot\nfig, ax = plot_perlin_art(perlin_data)\n\n\n\n\n\n\n\n\nLet’s take another look at the data transformations for the indices, i/max_n * scale and j/max_n * scale. First, the indices are divided by the largest dimension of the data - e.g., in a 50 x 100 matrix, all the indices would be divided by 100 before being passed to pnoise2. This transformation scales all the values so they are between 0 and 1, which means that we can use the n_x and n_y to change the size of the array - and therefore the plot’s resolution - without also changing the range of values passed to pnoise2. Lower dimensions will have the same pattern, but make the plot look pixelated:\n\n# Original - smooth\nperlin_data = gen_perlin_2d(n_x=1000, n_y=1000)\nfig, ax = plot_perlin_art(perlin_data)\n\n# Decreasing resolution\nperlin_data = gen_perlin_2d(n_x=50, n_y=50)\nfig, ax = plot_perlin_art(perlin_data)\n\n# Even more pixelated\nperlin_data = gen_perlin_2d(n_x=10, n_y=10)\nfig, ax = plot_perlin_art(perlin_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we want to change the magnitude of the values passed to pnoise2, we can use the function’s scale argument. This argument lets us create larger or smaller patterns:\n\n# Define dimensions of the data\nn_x = 1000\nn_y = 1000\n\n# Larger pattern with scale &lt; 1\nperlin_data = gen_perlin_2d(n_x, n_y, scale=0.1)\nfig, ax = plot_perlin_art(perlin_data)\n\n# Default scale (1)\nperlin_data = gen_perlin_2d(n_x, n_y, scale=1)\nfig, ax = plot_perlin_art(perlin_data)\n\n# Smaller patterns with scale &gt; 1\nperlin_data = gen_perlin_2d(n_x, n_y, scale=10)\nfig, ax = plot_perlin_art(perlin_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese function arguments - n_x, n_y, and scale - are examples of parameters that can be used to change the features of our art. While all of the plots above are made using the same basic rules, the different parameters can create drastically different effects. A large part of generative art is experimenting with different parameter values. In the next section we’ll add some more parameters to this system to change the generated Perlin noise."
  },
  {
    "objectID": "blog/intro_pyart1.html#adding-parameters",
    "href": "blog/intro_pyart1.html#adding-parameters",
    "title": "Introduction to generative art in Python using Perlin noise",
    "section": "Adding parameters",
    "text": "Adding parameters\nThe pnoise2 function has several built-in parameters that control the features of the generated Perlin noise. I’ve set up gen_perlin_2d so that any additional keyword arguments are passed to pnoise2, allowing us to easily modify these parameters.\nWe’ll experiment with octaves, lacunarity, persistence, and repeatx/repeaty. For generative art, it’s not crucial to understand all of the maths behind these parameters - when we experiment with lots of different parameters, the combined effects are often difficult to predict regardless. However, it’s helpful to know the different options and the ranges of values that produce interesting effects.\n\nOctaves\nWe can create interesting effects with Perlin noise by adding together Perlin noise with different characteristics. The octaves parameter controls the number of layers added together. By default, the spatial scale of the pattern (i.e., the size of the blobs) and the amplitude of the noise (i.e., the amount that the pattern contributes to the final effect) are both halved with each successive layer: you get smaller and smaller blobs, but the larger shapes remain the dominant pattern because they have relatively higher values.\nHere’s how increasing octaves impacts the pattern we’ve been working with:\n\n# Define dimensions of the data\nn_x = 1000\nn_y = 1000\n\n# One octave (default)\nperlin_data = gen_perlin_2d(n_x, n_y, octaves=1)\nfig, ax = plot_perlin_art(perlin_data)\n\n# Two octaves\nperlin_data = gen_perlin_2d(n_x, n_y, octaves=2)\nfig, ax = plot_perlin_art(perlin_data)\n\n# Four octaves\nperlin_data = gen_perlin_2d(n_x, n_y, octaves=4)\nfig, ax = plot_perlin_art(perlin_data)\n\n# Eight octaves\nperlin_data = gen_perlin_2d(n_x, n_y, octaves=8)\nfig, ax = plot_perlin_art(perlin_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe’re starting to get interesting, detailed patterns as the number of octaves increases. In fact, this is a fractal pattern: if you zoom in to part of the image, you see a pattern that look similar to the image as a whole.\nFor our generative art system, we need to know that octaves must be an integer greater than or equal to one. It’s also helpful to know that beyond a certain point, further increasing octaves doesn’t have a visible impact on the pattern, which makes sense: since the amplitude of the noise is halved with each layer, successive layers start to have miniscule effects on the generated data.\n\n\nLacunarity\nWhen we add together different layers of noise using the octaves parameter, we can also control how the noise changes from layer to layer using lacunarity. This parameter controls how the size of the blobs change with each layer, which is inversely related to the number of blobs (i.e., the “frequency”). The higher the lacunarity value, the more the blob sizes decrease with each new layer. As mentioned above, by default, lacunarity is 2.0 - the frequency doubles (which halves the spatial scale) with each successive layer.\nIt’s easier to see the impact of adjusting lacunarity when we keep octaves constant (here, 8):\n\n# Define dimensions of the data\nn_x = 1000\nn_y = 1000\n\n# Number of octaves\noctaves = 8\n\n# lacunarity=2 (default)\nperlin_data = gen_perlin_2d(n_x, n_y, octaves=octaves, lacunarity=2)\nfig, ax = plot_perlin_art(perlin_data)\n\n# lacunarity=3.25 (doesn't have to be an integer!)\nperlin_data = gen_perlin_2d(n_x, n_y, octaves=octaves, lacunarity=3.25)\nfig, ax = plot_perlin_art(perlin_data)\n\n# lacunarity=5\nperlin_data = gen_perlin_2d(n_x, n_y, octaves=octaves, lacunarity=5)\nfig, ax = plot_perlin_art(perlin_data)\n\n# lacunarity=10\nperlin_data = gen_perlin_2d(n_x, n_y, octaves=octaves, lacunarity=10)\nfig, ax = plot_perlin_art(perlin_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigher values tend to create busier patterns, which makes sense if we think about the fact that the amplitude of the noise also decays with each layer. If we’re adding smaller shapes to the image at an earlier stage - before the amplitude has had a chance to decay as much - they’ll have a greater impact on the final values.\nFor our system, values over one are the most interesting as they’ll add details to the image, and lacunarity does not have to be an integer.\n\n\nPersistence\nWe can also control how much each layer contributes to the final image using persistence. By default, persistence is 0.5, which halves the amplitude of the noise in each successive layer. If persistence is greater than 1, successive layers contribute more and more to final image, while less than 1 means that each later layer has less of an impact.\nHere’s the results of changing persistence while keeping octaves equal to 8:\n\n# Define dimensions of the data\nn_x = 1000\nn_y = 1000\n\n# Number of octaves\noctaves = 8\n\n# persistence=1.25\nperlin_data = gen_perlin_2d(n_x, n_y, octaves=octaves, persistence=1.25)\nfig, ax = plot_perlin_art(perlin_data)\n\n# persistence=1\nperlin_data = gen_perlin_2d(n_x, n_y, octaves=octaves, persistence=1)\nfig, ax = plot_perlin_art(perlin_data)\n\n# persistence=0.5 (default)\nperlin_data = gen_perlin_2d(n_x, n_y, octaves=octaves, persistence=0.5)\nfig, ax = plot_perlin_art(perlin_data)\n\n# persistence=0.25\nperlin_data = gen_perlin_2d(n_x, n_y, octaves=octaves, persistence=0.25)\nfig, ax = plot_perlin_art(perlin_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis parameter can be any value greater than zero. However, values too close to 0 or too far over 1 tend to have less interesting effects because individual layers either have too little or too much impact on the final image, respectively.\n\n\nTiling\nFinally, the repeatx and repeaty parameters allow us to repeat the noise pattern to create a tiled effect. Both parameters take the tile size as input, so we want to use a value that is less than scale (which is 1 by default).\n\n# Define dimensions of the data\nn_x = 1000\nn_y = 1000\n\n# Parameters\nscale = 2 # scale\noctaves = 8 # number of octaves\nn_tiles = 4 # number of tiles\n\n# Tiling both x and y (tile size = scale/n_tiles)\nperlin_data = gen_perlin_2d(\n  n_x, \n  n_y, \n  scale=scale, \n  octaves=octaves, \n  repeatx=scale/n_tiles, \n  repeaty=scale/n_tiles\n)\nfig, ax = plot_perlin_art(perlin_data)\n\n# Tiling x only\nperlin_data = gen_perlin_2d(\n  n_x, \n  n_y, \n  scale=scale, \n  octaves=octaves, \n  repeatx=scale/n_tiles\n)\nfig, ax = plot_perlin_art(perlin_data)\n\n# Tiling y only\nperlin_data = gen_perlin_2d(\n  n_x, \n  n_y, \n  scale=scale, \n  octaves=octaves, \n  repeaty=scale/n_tiles\n)\nfig, ax = plot_perlin_art(perlin_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf the tiles get too small, the transitions between tiles isn’t as smooth (which can also be an interesting effect). I’m not familiar with the maths behind this parameter, though, so you’ll have to experiment with the values to figure out where smooth tiles end!"
  },
  {
    "objectID": "blog/intro_pyart1.html#adding-randomness",
    "href": "blog/intro_pyart1.html#adding-randomness",
    "title": "Introduction to generative art in Python using Perlin noise",
    "section": "Adding randomness",
    "text": "Adding randomness\nOur parameters are like a recipe for our generative art. The parameters for baking chocolate chip cookies would be factors like\n\nthe ingredients\nhow much of each ingredient to use\nhow the ingredients are mixed\nthe amount of dough to use for each cookie\nthe location of the cookie on the baking tray\nthe oven temperature\nthe baking time\n\nHowever, even if you follow the exact same steps, the resulting cookies will not be identical from batch to batch. There will be slight differences outside of our control, such as the precise locations of the chocolate chips, the shape of each cookie as it spreads during baking, and the pattern of browning on the surface. This variability is due to randomness that is intrinsic to baking.\nIn the same way, we can get different outputs from our generative art system, even when we use the same parameters, because we can generate different versions of Perlin noise. In pnoise2, the base argument controls the version of Perlin noise that is generated. We also want to experiment with this value because some versions will produce subjectively nicer or prettier art, just by chance. So far we’ve just used pnoise2’s default base value of 1, which is why all of our outputs look kind of similar - even when the parameters are different.\nHere are different versions of Perlin noise generated using the same parameters:\n\n# Define dimensions of the data\nn_x = 1000\nn_y = 1000\n\n# Number of octaves\noctaves = 8\n\n# Number of versions to plot\nn_versions = 4\n\nfor i in range(n_versions):\n  # Use i as the base value\n  perlin_data = gen_perlin_2d(n_x, n_y, octaves=octaves, base=i)\n  fig, ax = plot_perlin_art(perlin_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that unlike pnoise2, many other functions for generating noise or random data do not produce the same output every time by default. In the “Scanning parameters” section, I’ll go over how to make these functions reproducible, too."
  },
  {
    "objectID": "blog/intro_pyart1.html#experimenting-with-colour",
    "href": "blog/intro_pyart1.html#experimenting-with-colour",
    "title": "Introduction to generative art in Python using Perlin noise",
    "section": "Experimenting with colour",
    "text": "Experimenting with colour\nColours can also be seen as parameters, but they deserve special attention since they have so much impact on the final output. So far we’ve been visualising our art using matplotlib’s default colourmap, “viridis”. Viridis is an example of a sequential colourmap - both the hue and lightness of the colours gradually change as the data’s values change. This relationship is easier to see if we turn on the colourbar:\n\n# Generate data\nperlin_data = gen_perlin_2d(n_x = 1000, n_y = 1000, octaves=8)\n\n# Plot the data as a heatmap with a colourbar\nim = plt.imshow(perlin_data)\nplt.colorbar(im) # colourbar\nplt.axis(\"off\") # remove axes\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHowever, our matplotlib plotting function, imshow, works with any of the built-in matplotlib colourmaps. Let’s add an argument to our plotting function plot_perlin_art that allows us to pass the colourmap to imshow.\n\ndef plot_perlin_art(perlin_data, cmap=\"viridis\", show_plot=True):\n  \n  # Plot the data as a heatmap\n  fig, ax = plt.subplots(1, 1)\n  ax.imshow(perlin_data, cmap)\n  ax.axis(\"off\") # remove axes\n  ax.set_aspect(\"equal\") # equal aspect ratio\n  if show_plot:\n    plt.show()\n  \n  # Return figure and axes handles\n  return fig, ax\n\nWe can now try out different colourmaps. We’ll use the same data that was generated above so any differences will just be due to changing the colours.\nHere’s a different sequential colourmap, “magma”:\n\n# A different sequential colourmap\nfig, ax = plot_perlin_art(perlin_data, cmap=\"magma\")\n\n\n\n\n\n\n\n\n\n\n\n\nIf this code was for a data visualisation progject, a sequential colourmap would probably make the most sense because we have continuous data with clear low and high values. In generative art, however, there are no rules. We can plot our Perlin noise using any type of colourmap:\n\n# Diverging - middle values are lightest, \n# lowest and highest are darkest (but different colours)\nfig, ax = plot_perlin_art(perlin_data, cmap=\"Spectral\")\n\n# Cyclic - colourmap loops around # (useful for cyclic data like time of day)\nfig, ax = plot_perlin_art(perlin_data, cmap=\"twilight\")\n\n# Qualitative (discrete values)\nfig, ax = plot_perlin_art(perlin_data, cmap=\"tab20b\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe qualitative colourmap in particular produces a very different effect: it maps a range of values onto each colour, producing bands of colours.\nWe’re also not limited to matplotlib’s colourmaps. There are many Python packages (such as palettable) that provide additional colourmaps, and we can also design our own:\n\n# Import classes for creating colourmaps\nfrom matplotlib.colors import LinearSegmentedColormap, ListedColormap\n\n# List of colours\nclrs = ['#d2a556', '#b55f59','#6a365a','#65456f','#5b5380','#4f628d','#446f95']\n\n# Qualitative colourmap (discrete values)\ncmap_qualitative = ListedColormap(clrs)\nfig, ax = plot_perlin_art(perlin_data, cmap=cmap_qualitative)\n\n# Continuous colourmap (adds many additional interpolated colours)\ncmap_continuous = LinearSegmentedColormap.from_list(name=\"my_cmap\", colors=clrs)\nfig, ax = plot_perlin_art(perlin_data, cmap=cmap_continuous)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese colourmaps wouldn’t be good for data visualisation: the qualitative colourmap has similar colours that are difficult to distinguish, and the continuous colourmap isn’t perceptually uniform. However, as I said before, the same rules don’t apply to art! In fact, using colours in atypical ways can create interesting effects - like the way the yellow and orange stands out from the background purple and blue colours in the plot above."
  },
  {
    "objectID": "blog/intro_pyart1.html#scanning-parameters",
    "href": "blog/intro_pyart1.html#scanning-parameters",
    "title": "Introduction to generative art in Python using Perlin noise",
    "section": "Scanning parameters",
    "text": "Scanning parameters\nWe now have many ways to produce different outputs from our generative art system. We could experiment with the different options by manually changing the inputs, like we have above. However, that approach can quickly get tedious, especially in more complex systems where the code takes more time to run. We can instead speed up this process by scanning the possible parameters.\nOne scan approach is a grid search - we choose possible values for each parameter, then use each combination of parameters. Here I’ve scanned lacunarity and persistence using a grid search, while keeping the other parameters fixed. I also change the base value for each output, as otherwise the underlying data will have a similar structure despite the parameter changes.\n\n# Values to scan for lacunarity and persistence\n# I've made the values evenly spaced here, but they do not have to be.\nscan_lacunarity = np.linspace(1.5, 3.5, 5)\nscan_persistence = np.linspace(0.1, 1.1, 5)\n\n# Fixed parameters\nn_x = 1000\nn_y = 1000\noctaves = 8\n\n# Custom continuous colourmap\nclrs = ['#d2a556', '#b55f59','#6a365a','#65456f','#5b5380','#4f628d','#446f95']\ncmap = ListedColormap(clrs)\n\n# Variable to use to set different base values \n# (so the randomly generated data is markedly different for each output)\nscan_count = 0 \n\n# Scan \nfor lacunarity in scan_lacunarity:\n  for persistence in scan_persistence:\n\n    # Generate data using specified parameters\n    perlin_data = gen_perlin_2d(\n      n_x=n_x, \n      n_y=n_y, \n      octaves=octaves, \n      lacunarity=lacunarity,\n      persistence=persistence,\n      base=scan_count\n    )\n\n    # Plot with custom colourmap\n    fig, ax = plot_perlin_art(perlin_data, cmap=cmap, show_plot=False)\n    \n    # Title with parameter values and base value\n    ax.set_title(f\"lacunarity={lacunarity}, persistence={persistence}, base={scan_count}\")\n    plt.show()\n\n    # Increment to change base for each iteration\n    scan_count += 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrid searches benefit from being exhaustive - we try out each combination of parameters - but they can quickly become impractical when there is a large number of parameters. Excluding colour, we have eight parameters in our simple system (octaves, lacunarity, persistence, scale, n_x, n_y, repeatx, and repeaty) - even testing out just five values for each parameter gives us 58 = 390,625 different combinations! Because we can only realistically test a small number of parameter values, we also miss all of the intermediate values.\nAnother option that helps address these issues is a random search where we randomly select the parameter values. We can draw from a discrete list of options, like in the grid search, or select from a continuous range of values, allowing us to see combinations that wouldn’t arise from our grid search. I also find random searches more fun because I have no idea what the next output will look like!\nHere’s a function that randomly selects lacunarity and persistence, as well as the base value. I’ve set the default parameter ranges to the same as our grid search so we can easily compare their results. Since lacunarity and persistence are floats, we can generate values for them using np.random.uniform. This numpy function draws from a uniform distribution, which means that all values between the specified lower and upper limits are equally likely to be chosen. For base we need an integer, so we instead use np.random.randint.\nUnlike our Perlin noise generator, np.random.uniform and np.random.randint will give us a different result each time they are run - however, we can make the results reproducible by setting a seed for numpy’s random number generator using np.random.seed.\n\n# Generate random parameters\ndef gen_perlin_art_param(\n  seed=0, # seed for reproducibility\n  lacunarity_l=1.5, # lower limit for lacunarity\n  lacunarity_h=3.5, # upper limit for lacunarity\n  persistence_l=0.1, # lower limit for persistence\n  persistence_h=1.1, # upper limit for persistence\n  base_l=1, # lower limit for base\n  base_h=99, # upper limit for base\n  octaves=8, # fixed parameters\n  n_x=1000,\n  n_y=1000\n):\n\n  # Dictionary for storing parameters\n  param = {}\n\n  # Set seed\n  np.random.seed(seed)\n\n  # Select lacunarity and persistence from uniform distributions with specified bounds\n  # Round to two decimal places so easier to print and since small changes do not greatly \n  # impact the results for these parameters \n  # (rounding may not be appropriate for other parameters)\n  param[\"lacunarity\"] = round(np.random.uniform(low=lacunarity_l, high=lacunarity_h),2)\n  param[\"persistence\"] = round(np.random.uniform(low=persistence_l, high=persistence_h),2)\n\n  # Randomly generate an integer for the base value\n  param[\"base\"] = np.random.randint(low=base_l, high=base_h)\n\n  # Add fixed parameters to dictionary\n  param[\"octaves\"] = octaves\n  param[\"n_x\"] = n_x\n  param[\"n_y\"] = n_y\n\n  return param\n\n\n\n\nUsing the seed argument, we can change the parameters generated or reproduce earlier parameters:\n\n# seed = 0\nparam = gen_perlin_art_param(seed=0)\nprint(param)\n\n# Different parameters with seed = 5\nparam = gen_perlin_art_param(seed=5)\nprint(param)\n\n# Returning to seed = 0 gives same results as before\nparam = gen_perlin_art_param(seed=0)\nprint(param)\n\n\n\n{'lacunarity': 2.6, 'persistence': 0.82, 'base': 68, 'octaves': 8, 'n_x': 1000, 'n_y': 1000}\n{'lacunarity': 1.94, 'persistence': 0.97, 'base': 17, 'octaves': 8, 'n_x': 1000, 'n_y': 1000}\n{'lacunarity': 2.6, 'persistence': 0.82, 'base': 68, 'octaves': 8, 'n_x': 1000, 'n_y': 1000}\n\n\n\n\nLet’s use this new function to generate 25 outputs from our system. We pass the dictionary containing the parameters to gen_perlin_2d using the ** prefix, which will unpack the dictionary and use the items as keyword arguments.\n\n# Number of plots to produce\nn_outputs = 25\n\n# Variable for shifting seed values\nstarting_seed = 1\n\n# Random scan\nfor i in range(n_outputs):\n  \n  # Generate parameters\n  seed_i = i + starting_seed\n  param = gen_perlin_art_param(seed=seed_i)\n  \n  # Generate data\n  perlin_data = gen_perlin_2d(**param)\n  \n  # Plot\n  fig, ax = plot_perlin_art(perlin_data, cmap=cmap, show_plot=False)\n  ax.set_title(\n    f\"seed={seed_i},\\n\"\n    f\"lacunarity={param['lacunarity']}, \"\n    f\"persistence = {param['persistence']}, \"\n    f\"base = {param['base']}\"\n  )\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEven though this search was random, we still get similar ranges of outputs as in our grid search, and we also see parameter values that weren’t possible in our original search.\nUsing our seed, we can reproduce any of these outputs (for example, if we wanted to save it at a higher resolution). Because we also output the parameters that result from that seed, we can also easily experiment with changing the parameters or using different colours. Let’s do so with seed = 7, which I think has an interesting pattern:\n\nseed = 7\n\n# Original version\nparam = gen_perlin_art_param(seed=seed)\nperlin_data = gen_perlin_2d(**param)\nfig, ax = plot_perlin_art(perlin_data, cmap=cmap)\n\n# Manually change the parameters to decrease persistence\nparam[\"persistence\"] = param[\"persistence\"] / 2 \nperlin_data = gen_perlin_2d(**param)\nfig, ax = plot_perlin_art(perlin_data, cmap=cmap)\n\n# Also change the colourmap\nfig, ax = plot_perlin_art(perlin_data, cmap=\"twilight\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen I explore one of my generative art systems, I always run multiple scans. I first use ranges of parameter values that look interesting based on my initial manual explorations. In subsequent scans, I’ll either widen the ranges to explore new parts of parameter space or narrow them to create more outputs from parameters that produce nice effects. You can even make lots of outputs from exactly the same parameter values by only changing the noise generated (e.g., using the base value in our system). Not all of the scan outputs will look nice or even reasonable, especially when pushing parameters to extreme values, but you’ll also often see surprising outputs that look completely different from your initial explorations.\nImportantly, changing the scan set up means that the random seeds from the previous scan will produce different parameters, so you can no longer use the seeds to make any earlier outputs. To reproduce all outputs, you’ll need to save the parameters used for each output or the code used for that scan (e.g., using version control or a function that you don’t modify)."
  },
  {
    "objectID": "blog/intro_pyart1.html#next-steps",
    "href": "blog/intro_pyart1.html#next-steps",
    "title": "Introduction to generative art in Python using Perlin noise",
    "section": "Next steps",
    "text": "Next steps\nWe now have a fledgling generative art system, but there are many ways we can extend or modify it. Here are some ideas to explore:\n\nAdd more parameters and the colourmap settings to the scan.\nCreate your own custom colourmaps.\nChange the colours and parameters to create natural patterns or shapes, like clouds or islands (see Yvan Scher’s tutorial and this code for inspiration).\n\nWe can also add a second data generation step (with additional parameters!) that modifies perlin_data using other approaches:\n\n# Generate Perlin data \nn_x = 1000\nn_y = 1000\nperlin_data = gen_perlin_2d(n_x=n_x, n_y=n_y, octaves=8)\n\n# Parameters for modifying Perlin data\nadd_high = np.max(abs(perlin_data)) * 1.5 # max amount that can be added\nadd_low = add_high/5 # min amount that can be added\n\n# Generate numbers to add to columns\nseed = 0 # seed for random number generation\nnp.random.seed(seed)\nvec = np.random.uniform(low=add_low, high=add_high, size=n_y)\n\n# Add to perlin_data (using broadcasting)\nperlin_data = perlin_data + vec\n\n# Plot\nfig, ax = plot_perlin_art(perlin_data)\n\n\n\n\n\n\n\n\n\n\n\n\nAdditionally, we can use Perlin noise in generative art without directly visualising the noise itself as a heatmap. Here’s a simple example that uses Perlin noise to set the colours and/or sizes of a grid of points:\n\n# Create grid of points to plot\nn_x = 25\nn_y = 25\nmax_n=max(n_x, n_y)\nscale = 1\nx = np.linspace(0, n_x, n_x) / (max_n * scale) # as in gen_perlin_2d, scale values between 0 and scale\ny = np.linspace(0, n_y, n_y) / (max_n * scale)\npoints_x, points_y = np.meshgrid(x, y) # grid\n\n# Get Perlin noise for each value in the grid\noctaves=8\npersistence=0.2\nbase=10\nperlin_data = np.zeros((n_y, n_x))\nfor i in range(n_y):\n  for j in range(n_x):\n    perlin_data[i, j] = noise.pnoise2(\n      points_x[i, j], points_y[i, j], octaves=octaves, persistence=persistence, base=base\n    )\n\n# Shift Perlin noise so &gt;= 0 so can use to control size\nmin_perlin = np.min(perlin_data)\nperlin_data = perlin_data + np.abs(min_perlin)\n\n# Figure settings\nsize_multiplier = 25 # multiplier for size of points\ncmap=\"twilight\" # colourmap\ndef apply_fig_settings(fig, ax):\n  fig.set_facecolor(\"#272727\") # background colour\n  ax.axis(\"off\") # remove axes\n  ax.set_aspect(\"equal\") # equal aspect ratio\n  plt.show()\n\n# Plot, setting size based on Perlin noise \nfig, ax = plt.subplots(1, 1)\nax.scatter(\n  points_x, \n  points_y, \n  s=(perlin_data * size_multiplier)\n)\napply_fig_settings(fig, ax)\n\n# Plot, setting colour based on Perlin noise \nfig, ax = plt.subplots(1, 1)\nax.scatter(\n  points_x, \n  points_y, \n  s=size_multiplier,\n  c=(perlin_data * size_multiplier),\n  cmap=cmap\n)\napply_fig_settings(fig, ax)\n\n# Plot, setting size and colour based on Perlin noise \nfig, ax = plt.subplots(1, 1)\nax.scatter(\n  points_x, \n  points_y, \n  s=(perlin_data * size_multiplier),\n  c=(perlin_data * size_multiplier),\n  cmap=cmap\n)\napply_fig_settings(fig, ax)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe uses for Perlin noise can also be more subtle. For example, in my Coastlines system, I used 1D Perlin noise to smoothly vary the transparency and distributions of the points that make up the narrow vertical lines.\nWe can also make improvements from a software engineering perspective; for example, you can\n\nCreate functions for scanning parameters.\nCreate a separate module for your generative art functions so they’re easy to reuse across different projects.\nDevelop a workflow for scanning parameters and saving the generated plots (make sure the scans are reproducible!).\n\nUsing good software engineering practices becomes crucial as systems become more complex - otherwise, it becomes difficult to explore the system and reproduce desired outputs.\nOn the subject of reproducibility, I used Python 3.11.9 for this tutorial with these package versions:\nmatplotlib==3.9.0\nnoise==1.2.2\nnumpy==2.0.0"
  },
  {
    "objectID": "blog/intro_pyart1.html#additional-resources",
    "href": "blog/intro_pyart1.html#additional-resources",
    "title": "Introduction to generative art in Python using Perlin noise",
    "section": "Additional resources",
    "text": "Additional resources\nDuring my search, I found a couple of introductory tutorials for generative art in Python:\n\nGeoffrey Bradway’s tutorial on creating art based on Voronoi diagrams\nNicola Rennie’s tutorial using another Python plotting library, plotnine\n\nThere are also lots of tutorials using turtle graphics, rather than scientific/data visualisation libraries.\nMost generative art advice is also applicable to any programming language - here are some of my favourite essays:\n\nBen Kovach: How to make generative art feel natural\nTyler Hobb’s essays"
  },
  {
    "objectID": "blog/intro_pyart1.html#license",
    "href": "blog/intro_pyart1.html#license",
    "title": "Introduction to generative art in Python using Perlin noise",
    "section": "License",
    "text": "License\nCC-BY"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Gabrielle M. Schroeder",
    "section": "",
    "text": "I am a research software engineer from Newcastle upon Tyne with a passion for uncovering and communicating insights from data.\nMy interest in data analysis was sparked by working with genetics data as an experimental biologist. In 2014, a Fulbright Postgraduate Award gave me the opportunity to transition to computational neuroscience, which included learning data science and programming. I now use those skills to develop data science and software solutions for a variety of research projects at Newcastle University.\nI particularly enjoy data visualisation, especially when it involves finding creative ways to tell stories or developing custom ways to explore complex data.\n\nEducation\nPhD in Computer Science | June 2022\nNewcastle University, Newcastle upon Tyne, UK\nMSc in Neuroinformatics | Dec. 2015\nNewcastle University, Newcastle Upon Tyne, UK\nBSc in Biological Sciences | May 2014\nRaleigh, North Carolina, US"
  },
  {
    "objectID": "data_vis.html",
    "href": "data_vis.html",
    "title": "Data visualisation",
    "section": "",
    "text": "I use personal projects to learn new tools, practice design concepts, and experiment with different visualisation techniques. Many of my projects are part of community initiatives such as Viz for Social Good (VFSG) and Tidy Tuesday.\nFor more examples, see my data science projects."
  },
  {
    "objectID": "data_vis.html#projects",
    "href": "data_vis.html#projects",
    "title": "Data visualisation",
    "section": "Projects",
    "text": "Projects"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Research software",
    "section": "",
    "text": "As a research software engineer (RSE) at Newcastle university, I develop software for a variety of research projects."
  },
  {
    "objectID": "software.html#projects",
    "href": "software.html#projects",
    "title": "Research software",
    "section": "Projects",
    "text": "Projects"
  },
  {
    "objectID": "data_vis/vfsg-gdri/index.html",
    "href": "data_vis/vfsg-gdri/index.html",
    "title": "Viz for Social Good: Global Deaf Research Institute",
    "section": "",
    "text": "I made these visualisations for the May 2024 Viz for Social Good project for the Global Deaf Research Institute (GDRI). GDRI asked for assistance with visualising data from an extensive pilot survey administered to over 200 deaf Nigerians."
  },
  {
    "objectID": "data_vis/vfsg-gdri/index.html#visualisations",
    "href": "data_vis/vfsg-gdri/index.html#visualisations",
    "title": "Viz for Social Good: Global Deaf Research Institute",
    "section": "Visualisations",
    "text": "Visualisations"
  },
  {
    "objectID": "data_vis/vfsg-gdri/index.html#data-story",
    "href": "data_vis/vfsg-gdri/index.html#data-story",
    "title": "Viz for Social Good: Global Deaf Research Institute",
    "section": "Data story",
    "text": "Data story\nGDRI’s survey contained over 100 questions with variable response rates. I chose to focus on some of the questions about sign language use and fluency. I was initially curious about whether sign language fluency impacted factors like quality of life or ease of communication (e.g., with healthcare professionals). However, after an initial data exploration I found that most of the respondents were fluent in sign language, and I did not think there was sufficient data on non-sign language users to answer these questions.\nInstead, I decided to focus on some other interesting observations from my analysis of the sign language variables: 1) the large proportion of sign language users (first visualisation), and 2) the gap between hearing loss and learning sign language (second and third visualisations). I also thought that these visualisations would be informative for refining future survey questions (see “Data recommendations”)."
  },
  {
    "objectID": "data_vis/vfsg-gdri/index.html#design-decisions",
    "href": "data_vis/vfsg-gdri/index.html#design-decisions",
    "title": "Viz for Social Good: Global Deaf Research Institute",
    "section": "Design decisions",
    "text": "Design decisions\nGDRI expressed a need for visualisations for multiple scenarios (e.g., communicating with stakeholders, presentations, funding bids). I therefore decided to make a series of smaller visualisations, rather than one large visualisation, to give GDRI more flexibility with how they use the visualisations. The layouts also allow my title and caption to be cropped out if GDRI would like to provide different context. Likewise, I stuck with a white background to make it easier to embed the visualisations in reports, presentations, and any printed materials.\nI used GDRI branding from their logo and website. I used their main colours (sky blue and dark greys) for most of the visual elements and selected a constrasting accent colour (dark mustard yellow) based on some of the photos on their website. The titles use their website font, Questrial. I paired this font with Lexend, which (to my beginner typographer eyes) shares many similar features with Questrial. Lexend has more open apertures than Questrial, however, making it easier to read at small font sizes, and it’s also generally designed for accessibility. Lexend also has more available font weights than Questrial, which is useful for emphasising subsets of text in annotations."
  },
  {
    "objectID": "data_vis/vfsg-gdri/index.html#data-decisions",
    "href": "data_vis/vfsg-gdri/index.html#data-decisions",
    "title": "Viz for Social Good: Global Deaf Research Institute",
    "section": "Data decisions",
    "text": "Data decisions\nFor my first visualisation on the number of sign language users, I combined information from the “Languages you use comfortably” and “How fluent are you in sign language?” variables due to limitations in the first variable - namely, many people who were fluent in sign language did not list a sign language in their languages (see “Data recommendations” below).\nI considered someone fluent in sign language if they listed sign language in their languages or if they had at least a “neither good nor bad” level of sign language fluency. I chose this fluency cutoff for two reasons: 1) it suggested a decent level of sign language knowledge, and 2) 8 people who listed this fluency level also listed sign language in “Languages you use comfortably”, suggesting it does provide a sufficient fluency level for their needs. Meanwhile, no one with lower fluency levels listed sign language in their languages.\nI changed all the text in the languages lists to lowercase to make text searching easier, then identified specific sign languages by looking for certain phrases (determined by looking through the possible responses):\n\nNSL: “nsl”, “n.s.l”, “nigeria sign language”, and “nigerian sign language”\nASL: “asl”, “a.s.l”, “america sign language”, “english language (sign language”, “english language of sign language”, “english sign language for the deaf”, or “sign language-english”\n\nI did not include “english sign language” alone as an ASL search string because in some cases, multiple languages were listed without punctuation (e.g., “english hausa fulani sign language”). As such, it wasn’t possible to determine if “english sign language” meant “ASL” or two separate languages (i.e., English and a sign language). This conservative approach potentially missed up to seven ASL users who used this phrasing.\nOtherwise, respondents who 1) listed an unspecified/indetermine sign language or 2) expressed fluency in sign language, but didn’t list a sign language in their languages, were marked as knowing an unspecified sign language."
  },
  {
    "objectID": "data_vis/vfsg-gdri/index.html#data-recommendations",
    "href": "data_vis/vfsg-gdri/index.html#data-recommendations",
    "title": "Viz for Social Good: Global Deaf Research Institute",
    "section": "Data recommendations",
    "text": "Data recommendations\nSince this data was collected as part of a pilot survey, GDRI also asked VFSG volunteers to share any recommendations for future data collection. After working with this data, I have a few recommendations:\n\nLanguage options\nGDRI’s survey asked an open-ended question, “Languages you use comfortably.” Interestingly, 94 (!) respondents who expressed high levels of sign language fluency (“good” or “very good”) did not list a sign language in their languages. Respondents may have interpretted this question as “Spoken/written languages you use comfortably”, used terms such as “English” to refer to both spoken and signed languages, or interpretted “comfortable” in a way besides fluency (e.g., they may be uncomfortable using sign language due to discrimination). Additionally, many sign language users did not specify the sign language. Therefore, to get a more complete list of languages that respondents can use, I would suggest providing a set of checkboxes for the region’s common spoken and signed languages in the survey. This format would also make the responses consistent and therefore easier to analyse.\n\n\nClear indication of not knowing sign language\nThe survey had no clear indication of whether a respondent did not know sign language. Because many people fluent in sign language did not list a sign language in their languages, the absence of a sign language in this list was not evidence for not knowing sign language. The sign language fluency question provides a better indication, but respondents who don’t know sign language could potential skip this question if they think it only pertains to sign language users. Providing the discrete language options, as suggested above, would provide stronger evidence for not knowing sign language, and a question such as “Do you know any sign language?” may also be a useful addition.\n\n\nWant to learn sign language\nGDRI also asked respondents whether they wanted to learn sign language. Although “I already know sign language” was a possible option, many people with high levels of sign language fluency answered “yes” or “no”, which is difficult to interpret (Are they expressing that they want to learn additional sign languages? Are they communicating that they wanted to learn sign language at an earlier time?). This question may need to be re-phrased or restricted to a subset of respondents (i.e., those who respond “no” to a direct question on whether they know sign language).\n\n\nHearing loss vs. “significant” hearing loss\nI observed a gap between when people began experiencing loss and when they learned sign language. One possible explanation is that the respondents could have had gradual hearing loss that did not initially require them to learn sign language for communication. I suspect this explanation is unlikely since many respondents began losing hearing at young ages, but a more definitive answer could be provided by additional data on respondents’ hearing loss. Namely, if this trend is interesting to GDRI’s researchers, it would be useful to know at what age the respondents had “significant” hearing loss according to some measure (e.g., no longer able to understand spoken language).\n\n\nDemographics\nDuring the project kick-off, GDRI emphasised that there are different deaf populations with different experiences and needs. For example, some people are born deaf and learn sign language at an early age, while others experience gradual hearing loss as they age and are not culturally deaf. These visualisations revealled that most of the survey respondents were sign language users who experienced hearing loss in infancy, childhood, or adolescence. As such, the conclusions from this survey will be specific to this demographic. In future surveys, GDRI could focus on this demographic or experiment with different recruitment methods to survey individuals with different characteristics."
  },
  {
    "objectID": "data_vis/vfsg-gdri/index.html#code",
    "href": "data_vis/vfsg-gdri/index.html#code",
    "title": "Viz for Social Good: Global Deaf Research Institute",
    "section": "Code",
    "text": "Code\nAll visualisations were made using the R programming language. Portions of the code can be reused for future GDRI research - however, it will need to be adapted to new data sets. For example, the data wrangling steps here address the needs of these particular survey responses; another survey, with different responses, may require different cleaning steps. Additional, some of the visualisations have “hard coded” elements (e.g., the positions of some of the annotations) that would need to be manually changed for other data sets. Additionally, other surveys will likely have different results that should be highlighted using different visualisation approaches or annotations!\nR for Data Science and the associated communities are great resources for learning how to use R for data analysis. The book covers many of the approaches I used here.\nThe code is available on GitHub. Code dependencies:\n\nGDRI data (add to the data folder)\nFont files for Questrial and Lexend. Functions in lib/lib_theme.R that use these files have an optional argument for specifying the path to your local directory that contains font files.\n\n\n\n\nLicense: CC BY-NC 4.0"
  },
  {
    "objectID": "data_vis/vfsg-dwf/index.html",
    "href": "data_vis/vfsg-dwf/index.html",
    "title": "Viz for Social Good: DWF ReconciliACTIONs",
    "section": "",
    "text": "This work was a volunteer project for Viz for Social Good (VFSG) in partnership with the Gord Downie & Chanie Wenjack Fund (DWF). The DWF aims to build cultural understanding and create a path towards reconciliation between Indigenous and non-Indigenous peoples in Canada. My visualisation was one of the top five designs selected by the DWF.\nCode for the data organisation, exploration, and visualisation is available on my GitHub."
  },
  {
    "objectID": "data_vis/vfsg-dwf/index.html#design-process",
    "href": "data_vis/vfsg-dwf/index.html#design-process",
    "title": "Viz for Social Good: DWF ReconciliACTIONs",
    "section": "Design process",
    "text": "Design process\nThis project was an opportunity to stretch my design skills and practice layout, typography, and colour design concepts. It was also my first time working with branding guidelines outside of academic journal figure requirements.\n\nTools\nR, Adobe Illustrator, and Adobe InDesign.\n\n\nStorytelling\nThe DWF asked for visualisations that allowed them to explore and communicate their impact, as measured by reconciliACTIONs: meaningful actions that progress reconciliation between Indigenous and non-Indigenous peoples in Canada. I decided to make a report in a style similar to their yearly reports that would summarise their progress and be easy to share with stakeholders.\n\n\n\n\n\n\n\nThe report first lays out definitions for steps and stages (part of the DWF’s model for measuring the impact of reconciliACTIONs), while visualising progress within each of these categories. These first visualisations and descriptions give the reader a big picture overview of the DWF’s progress as well as a foundation for understanding the DWF’s model of reconciliACTIONs.\n\n\n\n\n\n\n\n\n\n\n\n\nThe next pages then explore reconciliACTIONs using the combined categorisation into both steps and stages, with each stage broken down into the five possible reconciliACTION steps. One visualisation shows how the number of reconciliACTIONs in each combined category changed over time, while the other shows the contributions of different types of reconciliACTIONs, coloured by their categories.\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, I end with a call-to-action to “Do Something” to progress reconciliation by providing a template for tracking reconciliACTIONs. The latter was inspired by the DWF’s comment that they do not have reconciliACTION data at the level of individuals – i.e., they do not know which actions any one person has taken. I therefore wanted to invite people to track their own progress. I decided to distinguish between steps, but not stages, on the tracking template since it may be more difficult for non-experts to categorise their reconciliACTIONs by stages. Additionally, most people’s reconciliACTIONs will be in only one or two of the stages, at least in the near future.\n\n\n\n\n\n\n\nThe final page of the report provides short descriptions the DWF and VFSG, as well as ways for people to find out more information about these two organisations.\n\n\n\n\n\n\n\n\n\nColours and symbols\nI worked with the DWF’s brand colours, but I also expanded the colour palette using three main hues from the striking colour palette of one of their main resources, The Secret Path film. I used the cooler hues to represent earlier stages of reconciliACTIONs, allowing the warming of the colour palette to symbolise the progression of reconciliation. Meanwhile, each step within each stage is encoded by the lightness of the hue. The colour therefore darkens as the steps become more difficult, which corresponds nicely to the increased impact of the latter steps. The combined hue and lightness encoding allowed me to visualise the combined categorisation of each reconciliACTION in the report’s latter visualisations.\nI also incorporated imagery and art that is central to the DWF’s messaging and identity, including Secret Path lyrics, train tracks, and watercolour artwork.\n\n\nData decisions\nI decided to use the simpler DWF model of “stages” of reconciliACTIONs, rather than the categorisation into “future states” of Canada, which would have doubled the number of categories. Based on my initial data explorations, I felt that the simpler stages representation captured most of the variability in the types of reconciliACTIONs thus far, and I therefore sacrificed some level of detail to make the visualisations more accessible. Further, since the report is geared towards DWF supporters and stakeholders, I thought the stages description was most appropriate since it focuses on the development of individual participants in their path to reconciliation. However, I did tie the stages back to the future states description to explain how stages fit in to the DWF’s more detailed model.\nSome of the reconciliACTIONs had negative numbers due to a decrease in the net number of newsletter subscribers. As suggested by the DWF, I decided to transform these negative numbers to zero to prevent them from detracting from other reconciliACTIONs in the same category. My reasoning was that unsubscribing did not undo or negate the participant’s previous action of subscribing. Additionally, the unsubscribing participants may decide to continue their reconciliation journey using other approaches, such as following the DWF on social media. If possible, the DWF could separately track the number of new subscriptions and unsubscriptions in future quarters to have different options for tracking these reconciliACTIONs.\nI also combined newsletter subscribers and mailable newsletter subscribers into one category for the “Many Ways to #DoSomething” visualisation of different reconciliACTION types.\n\n\nBrainstorming\nI’ve always gravitated towards sketching out visualisation ideas, but – inspired by reading about graphic and visualisation designers’ processes – I decided to more systematically incorporate sketches into developing these visualisations. Throughout the project, I used sketches to test out different ideas for vis types, layouts, and encodings. I sketched out initial ideas after first listening to the DWF’s Viz for Social Good presentation, but I also made sure to look at the data early on to understand its limitations and the feasibility of my approach. For example, I saw early on that the different scales in the data would make it tricky to compare reconciliACTIONs across different categories. I returned to sketching throughout the project to refine my early ideas, overcome roadblocks, and test out new ideas."
  },
  {
    "objectID": "data_vis/vfsg-dwf/index.html#acknowledgments",
    "href": "data_vis/vfsg-dwf/index.html#acknowledgments",
    "title": "Viz for Social Good: DWF ReconciliACTIONs",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI used a slightly modified version of a generative art system designed by Danielle Navarro to make the irregular and smudged shapes in the #DoSomething tracking template. Her code is included in the art_functions.R script and is licensed under a Creative Commons Attribution 4.0 International License. The code is available from the Art from Code tutorial, session 3 and the associated GitHub page.\nDescriptions of the DWF and VFSG organisations are from their language guidelines and website, respectively."
  },
  {
    "objectID": "data_vis/macros-map/index.html#food-selection",
    "href": "data_vis/macros-map/index.html#food-selection",
    "title": "Macronutrients Map",
    "section": "Food selection",
    "text": "Food selection\nThe full USDA food database in the {NutrienTrackeR} package has an overwhelming number of foods: 7,754 . I pared this down to 1,530 foods using a combination of subsetting, heuristics, and a small amount of manual curation. I wanted to have a wide variety of foods while focusing on 1) less specific entries, 2) fresh and unprepared foods, and 3) foods with fewer ingredients (i.e., not complete meals or dishes). However, I kept some simpler processed and multi-ingredient foods, such as bread and yoghurt. I also removed some of the more unusual entries such as “raccoon.”\nMy wrangling steps included\n\nRemoving database categories that didn’t meet my criteria (e.g., “Fast Foods” and “Restaurant Foods”).\nRemoving entries with more than six words (excluding some phrases like “without salt” from the word count since they signified a simpler version of the food). This step removed very specific entries such as “Beef, chuck, shoulder clod, shoulder top and center steaks, separable lean and fat, trimmed to 0” fat, all grades, cooked, grilled” (21 words).\nRemoving alcoholic foods and beverages since they have a fourth macronutrient - alcohol - that I didn’t want to include in this visualisation. I identified these entries by estimating each food’s calories based on their carbs, fats, and protein content. I then found foods where this calculation greatly underestimated the true calorie content (since the remaining calories come from the alcohol). I removed any remaining alcohol-related entries using string matching to the food names.\nRemoving brand name foods. The USDA database capitalises most of the company names in the database, so these were easy to filter out by finding food names with multiple consecutive capital letters. I then manually filtered out some missed branded entries.\nRemoving entries that contained words that indicated cooking or processing - e.g., “cooked”, “roasted”, or “frozen”. I was careful here to keep longer versions of these strings that indicated the opposite - e.g., “uncooked”.\n\nTogether, these steps removed a lot of redundant entries, resulting in a more manageable map to explore.\nI also think this plot would work well using more tailored data, such as food from one person’s diet."
  },
  {
    "objectID": "data_vis/macros-map/index.html#layout",
    "href": "data_vis/macros-map/index.html#layout",
    "title": "Macronutrients Map",
    "section": "Layout",
    "text": "Layout\nThis visualisation uses a dimensionality technique called t-distributed Stochastic Neighbour Embedding (t-SNE), which embeds data in a small (e.g., two) number of dimensions to make it easier to visualise. The algorithm tries to optimise the new layout so that similar observations (here, foods) are placed close together. I based the similarity of the different foods off of their relative amounts of macronutrients and their calories per 100 g. Using relative macronutrient content (e.g., 20% protein) rather than the absolute macronutrient values (e.g., 10 g protein per 100 g) means that non-caloric content, such as water, is ignored for the embedding. I also normalised these features so that higher-magnitude features (such as calories) did not have a greater impact on food similarity.\nt-SNE is not deterministic - you can get different embeddings depending on where you first place each point. It also has a parameter, perplexity, that controls how much the algorithm focuses on nearby versus global similarities. To find an embedding that worked well for this visualisation, I systematically tried different starting configurations and perplexity values. I also experimented with scaling how much calories contributed to food similarity, as I wanted the layout to be primarily driven by the macronutrient content, with calories further separating foods with similar macronutrients. Ultimately, I divided the normalised calories by four to decrease their impact on the embedding.\nThe initial t-SNE embedding that I selected looked something like this:\n\n\n\n\n\nThis plot is pretty, but difficult to interact with because of the overlap between different foods, especially with the point size scaled by calories. I therefore used the R package {packcircles} to repel nearby points, resulting in a final layout without overlapping points. I also flipped the layout to work better with my text annotations - rotated or flipped embeddings are equivalent since they have the same distances between points. Here’s a non-interactive version of the final, annotated layout:\n\n\n\n\n\nWhile making this chart, I searched for any similar visualisations. I didn’t find any maps using these exact features, but there is a published static t-SNE embedding of USDA food data that uses the complete nutritional information (including micronutrient content such as vitamins). However, there are many more types of embedding approaches, so I may have missed other macronutrient representations - if you find any, I would love to see them!"
  },
  {
    "objectID": "data_vis/vfsg-noise-solution/index.html",
    "href": "data_vis/vfsg-noise-solution/index.html",
    "title": "Viz for Social Good: Noise Solution",
    "section": "",
    "text": "Despite being short on time this month, I couldn’t resist participating in VFSG’s project with Noise Solution, a charity based in England that uses music mentoring programmes to transform the lives of at risk youth.\nFor my submission, I focused on visualising how participants’ wellbeing changed from the start to the end of the programme:"
  },
  {
    "objectID": "data_vis/vfsg-noise-solution/index.html#design-choices",
    "href": "data_vis/vfsg-noise-solution/index.html#design-choices",
    "title": "Viz for Social Good: Noise Solution",
    "section": "Design choices",
    "text": "Design choices\nFor this project, I was inspired by the music theme to visualise the data in the shape of an old-school music player ring. Each point and line maps onto a set of Noise Solution sessions (and the corresponding participant), which helps emphasise the impact of the programme on individual people.\nNoise Solution’s branding also played a large role in my design. In addition to some of Noise Solution’s brand colours (purple, near-black, light grey, and white), I took inspiration from the photographs on their website, many of which have bright blues, teals, pinks, and oranges from coloured lights. Since I don’t have a license for Noise Solution’s brand font, Roc Grotesk, I used a similar font with an Open Font License, DM Sans.\nAs part of my design process, I created a mood board for inspiration:\n\n\n\n\n\nThe title references a music concept - matching a beat or rhythm - that also emphasises Noise Solution’s mission: by choosing to focus on mental wellbeing, they help their participants become more optimistic (or upbeat)."
  },
  {
    "objectID": "data_vis/vfsg-noise-solution/index.html#possible-next-steps",
    "href": "data_vis/vfsg-noise-solution/index.html#possible-next-steps",
    "title": "Viz for Social Good: Noise Solution",
    "section": "Possible next steps",
    "text": "Possible next steps\nOne of Noise Solution’s requests that I didn’t have time to fulfill was to make the visualisation interactive. There are some options for this approach in R (e.g., {plotly} and {shiny}), and I’m also keen to learn Javascript so that I can develop custom interactive visualisations for the web.\nWhile my chart choice emphasises the wellbeing change of each individual, it doesn’t accurately show the distribution of changes across all participants. As such, this visualisation would be complemented by an additional chart type that shows the distribution of wellbeing changes."
  },
  {
    "objectID": "data_vis/vfsg-noise-solution/index.html#tools",
    "href": "data_vis/vfsg-noise-solution/index.html#tools",
    "title": "Viz for Social Good: Noise Solution",
    "section": "Tools",
    "text": "Tools\nI created this visualisation using R (code here) and used Figma for the text and final layout."
  },
  {
    "objectID": "data_vis/vfsg-noise-solution/index.html#license",
    "href": "data_vis/vfsg-noise-solution/index.html#license",
    "title": "Viz for Social Good: Noise Solution",
    "section": "License",
    "text": "License\nThe code for this project is licensed under the MIT License. The visualisations and write-up are licensed under CC-BY-NC."
  },
  {
    "objectID": "data_vis/gardening-covid/index.html",
    "href": "data_vis/gardening-covid/index.html",
    "title": "Gardening’s growth during COVID",
    "section": "",
    "text": "These visualisations are a small personal project to practice using R and Datawrapper."
  },
  {
    "objectID": "data_vis/gardening-covid/index.html#gardenings-growth-during-the-covid-pandemic",
    "href": "data_vis/gardening-covid/index.html#gardenings-growth-during-the-covid-pandemic",
    "title": "Gardening’s growth during COVID",
    "section": "Gardening’s growth during the COVID pandemic",
    "text": "Gardening’s growth during the COVID pandemic\nWith the weather warming up in Newcastle upon Tyne, I’ve been spending more time in our new garden. As a novice gardener, I’ve also been doing more research online, Googling how and when to plant my various vegetable and flower seeds. These searches made me wonder how other gardeners use Google. Do their searches also vary over the course of the year?\nUsing Google Trends data, I began investigating garden searches in the UK, focusing on “how to garden” to capture interest in hobby gardening. I soon found that the expected seasonal changes weren’t the most interesting pattern in recent years:\n\n\n\n\n\n(Click on the chart for an interactive version.)\nCompared to the previous five years, 2020 saw more than double the number of “how to garden” searches after the first UK COVID-19 lockdown. In hindsight, the increased searches during 2020 are unsurprising, as the COVID-19 pandemic saw a dramatic increase in hobbiest gardeners. Like the pandemic, this higher interest persisted into 2021, with 2022 also seeing somewhat elevated searches. Interestingly, 2021 also bucks the usual seasonal pattern: “how to garden” searches increased a month earlier than expected, suggesting that gardeners were eager to get back outdoors after a winter of lockdowns!\nI can think of two main possibilities for the increased gardening interest:\n\nPeople looking for new at-home hobbies for themselves or their families. Many people had more free time due to furlough or working from home, and many usual hobbies and social activities were restricted by lockdowns.\nPeople who wanted access to fresh food without risking exposure to COVID-19 in shops or dealing with strange grocery delivery substitutions.\n\nTo better understand what drove the pandemic gardening Google searches, I investigated two more specific search terms: “how to grow vegetables” and “how to grow flowers.”\n\n\n\n\n\nBoth of these searches also dramatically increased during the first year of lockdowns. Although some flowers are edible or good companion plants, we can probably assume that flower searches were mostly hobby-motivated. Growing vegetables can also primarily be a hobby, but, interestingly, vegetable searches increased both sooner and more than flower searches in 2020. While home-grown crops take time to bear fruit (or veg), some searches may have been driven by initial lockdown fears or food order frustrations - especially if people suspected the pandemic would last beyond the first three week lockdown."
  },
  {
    "objectID": "data_vis/gardening-covid/index.html#bonus-stand-alone-r-visualisation",
    "href": "data_vis/gardening-covid/index.html#bonus-stand-alone-r-visualisation",
    "title": "Gardening’s growth during COVID",
    "section": "Bonus stand-alone R visualisation",
    "text": "Bonus stand-alone R visualisation\n\n\n\n\n\nHere I’ve added a third, more generic search: “how to grow food.” Food searches also increased during COVID, suggesting at least some gardening interest was driven by people who wanted to grow their own food."
  },
  {
    "objectID": "data_vis/tidy-tuesday/index.html",
    "href": "data_vis/tidy-tuesday/index.html",
    "title": "Tidy Tuesday (2023)",
    "section": "",
    "text": "These are my visualisations for Tidy Tuesday: a weekly R for Data Science data wrangling and visualisation challenge. Rough code for these visualisations is available on my GitHub.\n\nWeek 25: UFO Sightings\nI visualised when UFO sightings occur during the day and year. In the US, sightings tend to occur during the last few hours (during summer) or several hours (during winter) of the day, and sightings are especially common on Jan. 1 and July 4. The seasonal time-of-day variability in sightings may be because sightings tend to occur during dusk or the first part of the night.\n\n\n\n\n\n\n\n\n\n\nFewer UFO sightings were reported in the United Kingdom, but there are still some interesting trends:\n\n\n\n\n\n\n\nWeek 22: Verified Oldest People\nI visualised the years the oldest men and women lived and highlighted the people who set record ages."
  },
  {
    "objectID": "data_vis/vfsg-impact/index.html",
    "href": "data_vis/vfsg-impact/index.html",
    "title": "Viz for Social Good: VFSG Impact, 2017-2023",
    "section": "",
    "text": "This Viz For Social Good (VFSG) project focused on VFSG’s own data on volunteer submissions and non-profit partners. I chose to make data art of each past VFSG project, with the goal of creating engaging visualisations that also celebrate VFSG’s community and growth.\nI used the projects brief’s description of VFSG’s visualisations as “beacon[s] of change” as inspiration. For each project, the volunteers’ visualisations are represented by rays of light that illuminate the mission and impact of the project’s non-profit partner (represented by the layered geometric shape).\nHere are the pieces for the two projects I participated in last year (which are also two of my favourite outputs):"
  },
  {
    "objectID": "data_vis/vfsg-impact/index.html#how-to-read",
    "href": "data_vis/vfsg-impact/index.html#how-to-read",
    "title": "Viz for Social Good: VFSG Impact, 2017-2023",
    "section": "How to Read",
    "text": "How to Read\nData art is not designed to be the most effective at encoding or analysing data - for example, it’s difficult to directly compare the number of visualisations submitted for different projects using these illustrations. However, I wanted the data encoding to be apparent so each piece provides a general impression of the project’s data.\nEach submitted data visualisation is represented by one ray:\n\n\n\n\n\n\n\nThe corresponding non-profit for each project is represented by a geometric shape created by layering many regular polygons with the same number of sides. The number of sides is determined by the non-profit’s follower count on social media (more followers = more sides) as a rough approximation of its reach and recognisability. Smaller non-profits often benefit the most from VFSG’s work since they usually lack the resources for an internal data analysis team.\n\n\n\n\n\n\n\n\n\n\n\n\nTap Elderly Women’s Wisdom for Youth, with a little over a 100 followers on X (Twitter), vs. UNICEF, with over 9 million followers.\nThe colour of each shape is determined by the project’s primary topic. I grouped the provided topics into six broader categories:\n\nEducation (pink/orange)\nAmplifying voices (i.e., broadcasting the work and missions of other people or organisations across multiple categories) (yellow orange)\nInfrastructure, resources, and sustainability (yellow)\nEnvironment and conservation (teal/green)\nHealth (blue)\nWelfare, rights, and equality (purple)\n\nExamples of the above categories, from left to right:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(proper legend for the non-profits TBA!)"
  },
  {
    "objectID": "data_vis/vfsg-impact/index.html#highlighted-examples",
    "href": "data_vis/vfsg-impact/index.html#highlighted-examples",
    "title": "Viz for Social Good: VFSG Impact, 2017-2023",
    "section": "Highlighted examples",
    "text": "Highlighted examples\nPrevious projects using VFSG data:\n\n\n\n\n\n\n\n\n\n\n\n\nVFSG’s first and latest projects:\n\n\n\n\n\n\n\n\n\n\n\n\nThe project with the highest number of submissions:\n\n\n\n\n\n\n\nThe first project featuring presented visualisations:"
  },
  {
    "objectID": "data_vis/vfsg-impact/index.html#all-projects",
    "href": "data_vis/vfsg-impact/index.html#all-projects",
    "title": "Viz for Social Good: VFSG Impact, 2017-2023",
    "section": "All projects",
    "text": "All projects"
  },
  {
    "objectID": "data_vis/vfsg-impact/index.html#data-cleaning-and-analysis",
    "href": "data_vis/vfsg-impact/index.html#data-cleaning-and-analysis",
    "title": "Viz for Social Good: VFSG Impact, 2017-2023",
    "section": "Data cleaning and analysis",
    "text": "Data cleaning and analysis\nI used R (code in this GitHub repo) for the data analysis and visualisations. VFSG provided all of the data except for the non-profit social media follower counts.\n\nCollaborations\nSometimes, volunteers work together and submit a collaborative visualisation. I counted each unique collaboration as a separate “volunteer” when calculating volunteer submission numbers. However, a collaborative submission also counts towards the total number of submissions of the individual volunteers in the collaboration. This approach only makes a difference to the submission counts of volunteers who submitted a collaborative visualisation and then a solo visualisation for a later project.\n\n\nProject topics\nVFSG assigned 20 different topics to the 48 projects. To focus on high-level differences in project topics, I grouped these topics into six more general ones:\n\n“Environmental Impact” and “Conservation” were collapsed into Environment and conservation.\n“Homelessness,” “Financial support,” “Human Rights,” “Gender Equality,” “Crisis,” “Children and Youth,” and “Recycling” were were grouped into Welfare, rights, and equality. At first, “Recycling” might seem like a better fit for Environment and conservation. However, the “Recycling” project was Furniture Bank, whose primary focus is a welfare and equality challenge: ending furniture poverty.\nEducation and Health were already large, distinct categories, and I kept these in the simplified list. Health includes both the original “Health” and “Healthcare” topics.\nI combined “Energy/ Sustainability,” “Energy,” “Sanitation,” “Water management,” “Infrastructures,” and “Sustainable Development” into Infrastructure, resources, and sustainability.\nThe remaining original topics were “Data” (for past projects on VFSG’s impact) and “Community” (for projects with Video Volunteers and Fondation Follereau). Fondation Follereau’s project fit well in Welfare, rights, and equality. VFSG and Video Volunteers were both similar in that they help raise awareness of a range of issues by helping individuals and organisations. I therefore created a new category, Amplifying voices, to describe these projects.\n\nMany projects could belong to multiple categories, especially due to the interactions between many of these categories. Despite these limitations, these groupings provide a general impression of the types of organisations VFSG has collaborated with.\n\n\nFollower counts\nI manually collected each non-profit’s number of followers on X (Twitter) on 12 April, 2024, to provide a rough measure of the non-profit’s reach and level of recognition. The number of sides in each visualisation is a log-transformed and scaled version of the follower count. The Keith Richardson Foundation does not have an X account, so I used their Instagram follower count instead. This measure could be improved by summarising follower information from multiple social media accounts.\n\n\n\nLicense: CC BY-NC 4.0"
  },
  {
    "objectID": "data_vis/vfsg-who/index.html",
    "href": "data_vis/vfsg-who/index.html",
    "title": "Viz for Social Good: World Health Organisation",
    "section": "",
    "text": "I designed this visualisation for Viz for Social Good’s collaboration with the World Health Organisation (WHO). For this project, the WHO provided a dataset on global disabilities; their report on the data is available here. The WHO asked for impactful visualisations that raised awareness about disability. It was also key that the visualisation was accessible (e.g., by meeting accessibility standards). My visualisation was one of five (of 49) designs selected by the WHO for a panel presentation."
  },
  {
    "objectID": "data_vis/vfsg-who/index.html#anyone-can-have-a-disability",
    "href": "data_vis/vfsg-who/index.html#anyone-can-have-a-disability",
    "title": "Viz for Social Good: World Health Organisation",
    "section": "Anyone can have a disability",
    "text": "Anyone can have a disability\nThe visualisation above represents every 1 million people in the world with one small dot. About 8,000 dots are grouped together in a circle to represent the global population of almost 8 billion people. Approximately 1 in 6 of these dots (around 1,300 dots total) are highlighted to represent the 1.3 billion people living with disabilities worldwide.\nPeople with disabilities are diverse:\n\nThey can be any sex: 44% are male and 56% are female\nThey can be any age: 7% are 0 to 14 years old, 63% are 15 to 59 years old, and 30% are at least 60 years old\nAnd they live in countries with different income levels: 7% in low income, 40% in lower-middle income, 33% in upper-middle income, and 20% in high income countries\n\nThese factors all impact the health inequities experienced by people with disabilities.\nVisualisation sources:\n\nVisualisation by Gabrielle M. Schroeder\nData source: World Health Organisation (2021 global disability data)\nViz for Social Good volunteer project"
  },
  {
    "objectID": "data_vis/vfsg-who/index.html#story",
    "href": "data_vis/vfsg-who/index.html#story",
    "title": "Viz for Social Good: World Health Organisation",
    "section": "Story",
    "text": "Story\nI chose to present the key message that I took away from the WHO’s VFSG presentation: anyone can have a disability. In particular, disability is (1) common, and (2) impacts people with many different demographics. In other words, people with disabilities are a large and diverse group. I decided to keep the data analysis and statistics fairly simple to focus on this main message.\nAs such, I used the demographic information to answer the question, “What are the characteristics of people with disabilities?” (for example, what percentage of people with disabilities are female?) Importantly, my approach does not reveal the prevalence within each demographic, which could potentially lead to misinterpretations. For example, someone might assume that disability is less prevalent in high-income countries, even though high-income countries actually have the highest prevalence (see pages 23-24 of the WHO disability report). However, my visualisation can also correct erroneous assumptions. For example, although people who are 60+ years old are more likely to have a disability, the global population structure means that most people with disabilities are actually under 60 years old. Thus, these statistics can challenge stereotypes about people with disabilities.\nI also wanted to present this data in a memorable and impactful way. One of my first experiences seeing data visualisation used as a story-telling tool was the New York Time’s article on how race impacts economic mobility. Rather than just presenting bar charts or a Sankey diagram, the article uses animated points to represent the economic outcomes of individual people. Years later, I still remember that message because the data was connected to individuals. Inspired by that approach, I decided to try capturing the global scale and impact of disabilities by representing every one million people with one small point. Without animations, I don’t expect my visualisation to have the same impact as the New York Times charts, but my hope is that this visualisation will encourage people to sit with the data and mentally grasp the number of people living with disabilities. However, representing the data this way could also lead to accessibility issues if people struggle to perceive the small dots. I aimed to address these issues with other design choices (discussed below)."
  },
  {
    "objectID": "data_vis/vfsg-who/index.html#accessibility",
    "href": "data_vis/vfsg-who/index.html#accessibility",
    "title": "Viz for Social Good: World Health Organisation",
    "section": "Accessibility",
    "text": "Accessibility\n\nData encoding\nAs I described above, I decided to represent one million people with a small dot. I then coloured these dots based on the people’s characteristics - e.g., whether they have disabilities. Since these small elements could be difficult for people with visual impairments to perceive, I ensured that all of the statistics were also encoded by the shapes formed by the dots:\n\nWhile the dots represented by people with disabilities are mixed into the circle that forms the global population, I extract those dots into a second, smaller circle that represents people with disabilities. The relative areas of those two circles encode the percentage of people with disabilities: the people with disabilities circle is approximately 1/6th the area of the global population circle.\nThe demographic information forms stacked bar charts, with the length of each section encoding the corresponding percentage. This double encoding should help everyone understand the data, as it’s much easier to perceive differences in lengths than differences in dot numbers.\n\n\n\nColour\nI avoided using the most problematic colour combinations for people with colour vision deficiencies, but I didn’t want to rely on that approach, especially since there are many different types of deficiencies. Therefore, to make my visualisation accessible, I also ensured that (1) neighbouring colours were differentiated by lightness as well as hue, and (2) no data encodings relied on colour alone. For example, each bar chart has vertical lines marking the boundaries between groups.\nTo check the impact of my colour choices on accessibility, I used Adobe’s colourblind checker and the Colbis (Color Blindness Simulator).\n\n\nFont, text, and contrast\nAfter researching factors that impact font accessibility, I decided to use Atkinson Hyperlegible, a font designed by the Braille Institute for people with low vision. It’s also freely available under the Open Font License, which was one of my requirements. Atkinson Hyperlegible uses extra space and exaggerated letterforms to make it easier to distinguish similar characters. The designed asymmetries also make it easier to distinguish letters that are mirror images of each other, such as p and q (e.g., compare the q in “inequities” vs. the p in “people” in my visualisation). The font was designed to be suitable for general audiences as well, and the distinct letterforms can improve readability for many people.\nAll of my text is either dark grey or purple that meets the WCAG 2.1 AAA standards for contrast against the white background (checked using Adobe’s accessibility tools). I also checked the contrast of the purple dots against the grey dots in the “global population” circle.\nThe size of all of the text is 10+ pt when the image width is 6.5+ inches, with the most important messages in larger font sizes. The visualisation has a high resolution so that it can be viewed at a larger size if desired.\n\n\nData accessibility\nThe image has the following alt text:\nA visualisation that represents every 1 million people in the world with one dot to show that 1 in 6 people, or 1.3 billion people worldwide, have a significant disability. To show the diversity of people with disabilities, the dots are rearranged into three stacked bar charts that provide sex, age, and country income level demographics.\nOne downside of using R to make a static visualisation is that, to my knowledge, there isn’t a way to allow screen readers to navigate the different chart elements. To make sure the key messages are still accessible, I also provide a text version of the main messages and statistics below the visualisation."
  },
  {
    "objectID": "data_vis/vfsg-who/index.html#other-design-choices",
    "href": "data_vis/vfsg-who/index.html#other-design-choices",
    "title": "Viz for Social Good: World Health Organisation",
    "section": "Other design choices",
    "text": "Other design choices\n\nColour significance\nDuring this project, I learned that there are many colours used to represent different types of disability. I chose purple, which is becoming a symbol of disability as a whole.\n\n\nTool\nI made this visualisation using the R programming language, both for personal reasons (wanting to practice using the tidyverse/ggplot2 packages) and to have the large amount of control over the plot and text that R provides. The code is available on my GitHub.\n\n\n\nLicense: CC BY-NC 4.0"
  },
  {
    "objectID": "gen_art.html",
    "href": "gen_art.html",
    "title": "Generative art",
    "section": "",
    "text": "I started experimenting with generative art while learning R. These pieces were created by programs that generate data using specific rules. By modifying the settings and incorporating randomness, the programs can produce qualitatively different outputs. I enjoy discovering the variety of outputs a system can create within its constraints.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoastlines\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApertures\n\n\n\nR\n\n\nambient\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDrifts\n\n\n\nR\n\n\nambient\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "data_science.html",
    "href": "data_science.html",
    "title": "Data science",
    "section": "",
    "text": "As a researcher in the Computational Neurology, Neuroscience, and Psychiatry Lab, I used a variety of data science techniques to uncover patterns in epileptic brain activity. More information about these projects is available in the corresponding open access papers."
  },
  {
    "objectID": "data_science.html#projects",
    "href": "data_science.html#projects",
    "title": "Data science",
    "section": "Projects",
    "text": "Projects"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Introduction to generative art in Python using Perlin noise\n\n\n\n\n\n\n\ngenerative art\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2024\n\n\n\n\n\n\nNo matching items"
  }
]